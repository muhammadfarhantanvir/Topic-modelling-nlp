{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67ddfd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T06:02:37.769698Z",
     "start_time": "2023-09-19T06:02:18.469326Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b6b59c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T06:02:49.137386Z",
     "start_time": "2023-09-19T06:02:48.249857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>Sales</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SOM_118_00055_FA163EF3DB12-1b43-e3615700-3dca9...</td>\n",
       "      <td>brot urlaub ihre mein name wie kann ich weiter...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SOM_TPF_00101_FA163E22F5B7-1af3-de819700-4197c...</td>\n",
       "      <td>einen schönen guten tag lauditag du stehst ja ...</td>\n",
       "      <td>0</td>\n",
       "      <td>lauditag stehst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SOM_141_00085_FA163E622DEB-1b45-68d0e700-3c9a0...</td>\n",
       "      <td>guten tag und herzlich willkommen bei o sie sp...</td>\n",
       "      <td>0</td>\n",
       "      <td>eta kumpel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SOM_LUB_00112_FA163E22F5B7-1af3-dc815700-45c46...</td>\n",
       "      <td>hallo herzlich willkommen bei der service sie ...</td>\n",
       "      <td>0</td>\n",
       "      <td>bert bislang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SOM_AMV_00100_FA163E153AE3-1bb7-3f731700-3fe36...</td>\n",
       "      <td>hallo schönen guten tag mein name die firma ot...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>395</td>\n",
       "      <td>SOM_LUB_00246_FA163EF3DB12-1b43-e3615700-3be4f...</td>\n",
       "      <td>herzlich willkommen bei dem otto service sie s...</td>\n",
       "      <td>0</td>\n",
       "      <td>pending eingeschränkt eingeschränkt vorteilen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>396</td>\n",
       "      <td>SOM_VYD_00630_FA163ED88855-1bef-b9321700-412dc...</td>\n",
       "      <td>herzlich willkommen bei o mein name wie kann i...</td>\n",
       "      <td>0</td>\n",
       "      <td>bahnhofstraße bahnhof bahnhofstraße ttt inhous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>397</td>\n",
       "      <td>SOM_LUB_00379_FA163E622DEB-1b45-6850d700-3e961...</td>\n",
       "      <td>herzlich willkommen bei o sie sprechen mit ihr...</td>\n",
       "      <td>0</td>\n",
       "      <td>rücksetzung aufgehängt rücksetzung frühen gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>398</td>\n",
       "      <td>SOM_TPF_00515_FA163E56E95C-1b1d-4929d700-3aab3...</td>\n",
       "      <td>herzlich willkommen bei blau ich werde am buck...</td>\n",
       "      <td>1</td>\n",
       "      <td>mso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>399</td>\n",
       "      <td>SOM_TPF_00410_FA163E622DEB-1b45-6850d700-40380...</td>\n",
       "      <td>herzlich willkommen bei ihrem o service sie sp...</td>\n",
       "      <td>0</td>\n",
       "      <td>warner sofia sofia vollzogen sofia sophia stoh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1998 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                          file_name  \\\n",
       "0              0  SOM_118_00055_FA163EF3DB12-1b43-e3615700-3dca9...   \n",
       "1              1  SOM_TPF_00101_FA163E22F5B7-1af3-de819700-4197c...   \n",
       "2              2  SOM_141_00085_FA163E622DEB-1b45-68d0e700-3c9a0...   \n",
       "3              3  SOM_LUB_00112_FA163E22F5B7-1af3-dc815700-45c46...   \n",
       "4              4  SOM_AMV_00100_FA163E153AE3-1bb7-3f731700-3fe36...   \n",
       "...          ...                                                ...   \n",
       "1993         395  SOM_LUB_00246_FA163EF3DB12-1b43-e3615700-3be4f...   \n",
       "1994         396  SOM_VYD_00630_FA163ED88855-1bef-b9321700-412dc...   \n",
       "1995         397  SOM_LUB_00379_FA163E622DEB-1b45-6850d700-3e961...   \n",
       "1996         398  SOM_TPF_00515_FA163E56E95C-1b1d-4929d700-3aab3...   \n",
       "1997         399  SOM_TPF_00410_FA163E622DEB-1b45-6850d700-40380...   \n",
       "\n",
       "                                                   text  Sales  \\\n",
       "0     brot urlaub ihre mein name wie kann ich weiter...      0   \n",
       "1     einen schönen guten tag lauditag du stehst ja ...      0   \n",
       "2     guten tag und herzlich willkommen bei o sie sp...      0   \n",
       "3     hallo herzlich willkommen bei der service sie ...      0   \n",
       "4     hallo schönen guten tag mein name die firma ot...      0   \n",
       "...                                                 ...    ...   \n",
       "1993  herzlich willkommen bei dem otto service sie s...      0   \n",
       "1994  herzlich willkommen bei o mein name wie kann i...      0   \n",
       "1995  herzlich willkommen bei o sie sprechen mit ihr...      0   \n",
       "1996  herzlich willkommen bei blau ich werde am buck...      1   \n",
       "1997  herzlich willkommen bei ihrem o service sie sp...      0   \n",
       "\n",
       "                                          filtered_text  \n",
       "0                                                   NaN  \n",
       "1                                       lauditag stehst  \n",
       "2                                            eta kumpel  \n",
       "3                                          bert bislang  \n",
       "4                                                   NaN  \n",
       "...                                                 ...  \n",
       "1993      pending eingeschränkt eingeschränkt vorteilen  \n",
       "1994  bahnhofstraße bahnhof bahnhofstraße ttt inhous...  \n",
       "1995  rücksetzung aufgehängt rücksetzung frühen gene...  \n",
       "1996                                                mso  \n",
       "1997  warner sofia sofia vollzogen sofia sophia stoh...  \n",
       "\n",
       "[1998 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your German text dataset\n",
    "# Replace 'your_dataset.csv' with the path to your dataset file\n",
    "df = pd.read_excel('labeled_data_whole.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912f3f42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T06:04:52.371961Z",
     "start_time": "2023-09-19T06:02:55.709403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>Sales</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SOM_118_00055_FA163EF3DB12-1b43-e3615700-3dca9...</td>\n",
       "      <td>brot urlaub ihre mein name wie kann ich weiter...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SOM_TPF_00101_FA163E22F5B7-1af3-de819700-4197c...</td>\n",
       "      <td>einen schönen guten tag lauditag du stehst ja ...</td>\n",
       "      <td>0</td>\n",
       "      <td>lauditag stehst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SOM_141_00085_FA163E622DEB-1b45-68d0e700-3c9a0...</td>\n",
       "      <td>guten tag und herzlich willkommen bei sie spre...</td>\n",
       "      <td>0</td>\n",
       "      <td>eta kumpel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SOM_LUB_00112_FA163E22F5B7-1af3-dc815700-45c46...</td>\n",
       "      <td>herzlich willkommen bei der service sie sprech...</td>\n",
       "      <td>0</td>\n",
       "      <td>bert bislang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SOM_AMV_00100_FA163E153AE3-1bb7-3f731700-3fe36...</td>\n",
       "      <td>hallo schönen guten tag mein name die firma wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          file_name  \\\n",
       "0           0  SOM_118_00055_FA163EF3DB12-1b43-e3615700-3dca9...   \n",
       "1           1  SOM_TPF_00101_FA163E22F5B7-1af3-de819700-4197c...   \n",
       "2           2  SOM_141_00085_FA163E622DEB-1b45-68d0e700-3c9a0...   \n",
       "3           3  SOM_LUB_00112_FA163E22F5B7-1af3-dc815700-45c46...   \n",
       "4           4  SOM_AMV_00100_FA163E153AE3-1bb7-3f731700-3fe36...   \n",
       "\n",
       "                                                text  Sales    filtered_text  \n",
       "0  brot urlaub ihre mein name wie kann ich weiter...      0              NaN  \n",
       "1  einen schönen guten tag lauditag du stehst ja ...      0  lauditag stehst  \n",
       "2  guten tag und herzlich willkommen bei sie spre...      0       eta kumpel  \n",
       "3  herzlich willkommen bei der service sie sprech...      0     bert bislang  \n",
       "4  hallo schönen guten tag mein name die firma wa...      0              NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the spaCy German language model\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "# Create an empty list to store the updated text without proper names\n",
    "updated_texts = []\n",
    "# Iterate over the text column and remove proper names using spaCy\n",
    "for text in df['text']:\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.pos_ != 'PROPN':\n",
    "            tokens.append(token.text)\n",
    "    updated_texts.append(' '.join(tokens))\n",
    "# Update the 'text' column with the modified text\n",
    "df['text'] = updated_texts\n",
    "\n",
    "# Print the updated dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a188e385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T06:05:37.617781Z",
     "start_time": "2023-09-19T06:05:31.329326Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Muhammad\n",
      "[nltk_data]     Farhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Muhammad\n",
      "[nltk_data]     Farhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Muhammad\n",
      "[nltk_data]     Farhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       brot urlaub nam weiterhelf leid verkehrt leit ...\n",
      "1       schon gut tag lauditag steh nam helf geht rufn...\n",
      "2       gut tag herzlich willkomm sprech leid helf bev...\n",
      "3       herzlich willkomm servic sprech domin tun kund...\n",
      "4       hallo schon gut tag nam firma vorlieg wann wan...\n",
      "                              ...                        \n",
      "1993    herzlich willkomm servic sprech tun kart momen...\n",
      "1994    herzlich willkomm nam weiterhelf kurz person k...\n",
      "1995    herzlich willkomm sprech tarifberat plan tun m...\n",
      "1996    herzlich willkomm blau buckel nam darf tun bit...\n",
      "1997    herzlich willkomm servic sprech darf tun frau ...\n",
      "Name: text, Length: 1998, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Download the NLTK stop words list and wordnet for lemmatization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the Snowball stemmer and WordNet lemmatizer\n",
    "stemmer = SnowballStemmer(\"german\")  # Use the appropriate language for stemming\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('german'))  # Use the appropriate language\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Remove short words (length < 3 characters)\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    \n",
    "    # Lemmatize or stem the words\n",
    "    # Uncomment one of the following lines based on your choice\n",
    "    # tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    tokens = [stemmer.stem(word) for word in tokens]  # Stemming\n",
    "    \n",
    "    # Join the cleaned tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the preprocessing function to the DataFrame\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "print(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b02da78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T06:12:05.156008Z",
     "start_time": "2023-09-19T06:12:04.850013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       urlaub weiterhelf verkehrt leit direkt aufleg ...\n",
      "1       lauditag steh helf end prepaid registri hinter...\n",
      "2       sprech helf bevor erklar nenn selbststand leit...\n",
      "3       servic sprech domin kundennumm hatt sicherheit...\n",
      "4       hallo firma vorlieg wann wann bestellt komisch...\n",
      "                              ...                        \n",
      "1993    servic sprech anruf entschuld angegeb end test...\n",
      "1994    weiterhelf kennzahl nenn wann termin ungefahr ...\n",
      "1995    sprech tarifberat plan momentch aktuell wollt ...\n",
      "1996    blau buckel telefonnumm wiederhol vielmal adre...\n",
      "1997    servic sprech sofia verstand einzelverbindungs...\n",
      "Name: text, Length: 1998, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Words to remove\n",
    "words_to_remove = ['bitt', 'mal', 'schon', 'genau', 'dank', 'gut', 'tag', 'numm', 'person', 'kurz', \n",
    "                   'einfach', 'neu', 'schau', 'moment', 'sag', 'muss', 'geht', 'fall', 'ganz', 'war', 'gern', 'frag', \n",
    "                   'gleich',\n",
    "                  'guck', 'imm', 'halt', 'wurd', 'seh', 'natur', 'quasi', 'gerad', 'bekomm', 'rout', 'gesagt', 'nein', \n",
    "                   'tun', 'syst',\n",
    "                  'mehr', 'mocht', 'box', 'geh', 'zugangsdat', 'passwort', 'kennwort', 'steht','seit', 'klein', 'bekomm', 'rufnumm', \n",
    "                   'kundenkennzahl', 'leid', 'tun', 'nam', 'per', 'willkomm', 'herzlich', 'gemacht', 'bleib', 'herr',\n",
    "                   'kolleg',\n",
    "                   'schick', 'frau', 'geb', 'richtig', 'vielleicht', 'viel','kart', 'brot', 'brauch',\n",
    "                  'erst', 'gibt', 'kommt', 'moglich',  'pro', 'iphon', 'sekund', 'minut', 'wirklich', 'krieg',\n",
    "                   'tatsach', 'musst', 'weiss',  'darf', 'find', 'gar', 'probl' , 'nee', 'klar']\n",
    "\n",
    "# Function to remove specified words from text\n",
    "def remove_specific_words(text):\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Filter out words to remove\n",
    "    filtered_tokens = [word for word in tokens if word not in words_to_remove]\n",
    "    \n",
    "    # Join the remaining tokens back into a single string\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the function to the 'cleaned_text' column\n",
    "df['text'] = df['text'].apply(remove_specific_words)\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "print(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b21ac2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T06:12:10.870365Z",
     "start_time": "2023-09-19T06:12:10.730928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Topic Modeling (LDA)\n",
    "# Convert text data to a document-term matrix\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bf9211e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T06:12:20.802284Z",
     "start_time": "2023-09-19T06:12:13.495594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for each topic:\n",
      "Topic 1: monat, tarif, vertrag, angebot, rabatt, handy, kostet, jahr, bezahl, beispiel\n",
      "Topic 2: technik, storung, leitung, funktioniert, ticket, anruf, uhr, desweg, adress, versuch\n",
      "Topic 3: vertrag, rechnung, kundennumm, monat, wunsch, end, wiss, versteh, handy, letzt\n",
      "Topic 4: vertrag, monat, auftrag, weit, bestat, erhalt, widerrufsbelehr, buch, vertragszusammenfass, sup\n",
      "Topic 5: post, kundig, vertrag, blau, helf, aktivi, bestat, wart, anruf, soll\n",
      "Topic 6: versteh, vertrag, adress, storniert, wart, selb, desweg, word, handy, sprech\n",
      "Topic 7: sms, bewert, wunsch, formular, gesprach, vertrag, hallo, anruf, portier, anbiet\n"
     ]
    }
   ],
   "source": [
    "# Apply LDA for topic modeling\n",
    "num_topics = 7  # Adjust the number of topics as needed\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Get the top words for each topic\n",
    "def get_top_words(model, vectorizer, n_words=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_word_idx = topic.argsort()[:-n_words - 1:-1]\n",
    "        top_words = [words[i] for i in top_word_idx]\n",
    "        topics[f\"Topic {topic_idx + 1}\"] = top_words\n",
    "    return topics\n",
    "\n",
    "topics = get_top_words(lda, vectorizer)\n",
    "print(\"Top words for each topic:\")\n",
    "for topic, top_words in topics.items():\n",
    "    print(f\"{topic}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa1148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
